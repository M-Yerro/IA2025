{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-Yerro/IA2025/blob/main/24-IA2025%20MicroTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hola! Aquí hay un mini transformer para explorar. Es un fork del original de Andrej Karpathy."
      ],
      "metadata": {
        "id": "HdP9Od045ZFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT. (https://www.youtube.com/watch?v=kCc8FmEb1nY)"
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "43149f2a-ae4f-4a86-fa4a-53b8513b410f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-20 01:01:37--  https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1060259 (1.0M) [text/plain]\n",
            "Saving to: ‘el_quijote.txt.2’\n",
            "\n",
            "\rel_quijote.txt.2      0%[                    ]       0  --.-KB/s               \rel_quijote.txt.2    100%[===================>]   1.01M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-20 01:01:37 (33.3 MB/s) - ‘el_quijote.txt.2’ saved [1060259/1060259]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Vamos a usar un dataset... y no es MNIST!\n",
        "!wget https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo abrimos\n",
        "with open('el_quijote.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Largo del dataset en caracteres: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "b627e157-4799-4b37-8bab-ff5bfdfc84cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largo del dataset en caracteres:  1038397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miramos los primeros mil\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "edd0a02f-92a3-4091-84b5-7e13db97400d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON QUIJOTE DE LA MANCHA\n",
            "Miguel de Cervantes Saavedra\n",
            "\n",
            "PRIMERA PARTE\n",
            "CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lentejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda. El resto della concluían sayo de velarte, calzas de velludo para las fiestas con sus pantuflos de lo mismo, los días de entre semana se honraba con su vellori de lo más fino. Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años, era de complexión recia, sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qué caracteres usa?\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "a0411d41-4684-4717-b22e-28fe37391372"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-.0123456789:;<?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijlmnopqrstuvxyz¡«»¿̀́̃̈–‘’“”\n",
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapeamos los caracteres a enteros\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: entra string, salen integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: entran integers, sale string\n",
        "\n",
        "print(encode(\"Buenas, como va?\"))\n",
        "print(decode(encode(\"Buenas, como va?\")))\n",
        "\n",
        "print(encode(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "54b2e4b3-79aa-4940-f18a-df9aa599f9ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25, 71, 56, 64, 52, 69, 7, 1, 54, 65, 63, 65, 1, 72, 52, 23]\n",
            "Buenas, como va?\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# codificamos todo el dataset a integers y lo metemos en un torch.Tensor\n",
        "import torch # Sí, vamos a usar Pytorch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # Nuestro GPT verá los primeros mil caracteres así:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "9d2838d5-ba09-4a4c-cb5a-89abdcb7625e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1038397]) torch.int64\n",
            "tensor([27, 38, 37,  1, 40, 44, 32, 33, 38, 43, 28,  1, 27, 28,  1, 35, 24,  1,\n",
            "        36, 24, 37, 26, 31, 24,  0, 36, 60, 58, 71, 56, 62,  1, 55, 56,  1, 26,\n",
            "        56, 68, 72, 52, 64, 70, 56, 69,  1, 42, 52, 52, 72, 56, 55, 68, 52,  0,\n",
            "         0, 39, 41, 32, 36, 28, 41, 24,  1, 39, 24, 41, 43, 28,  0, 26, 24, 39,\n",
            "        32, 81, 43, 44, 35, 38,  1, 11, 20,  1, 40, 71, 56,  1, 70, 68, 52, 70,\n",
            "        52,  1, 55, 56,  1, 62, 52,  1, 54, 65, 64, 55, 60, 54, 60, 65, 81, 64,\n",
            "         1, 74,  1, 56, 61, 56, 68, 54, 60, 54, 60, 65,  1, 55, 56, 62,  1, 57,\n",
            "        52, 63, 65, 69, 65,  1, 59, 60, 55, 52, 62, 58, 65,  1, 27,  9,  1, 40,\n",
            "        71, 60, 61, 65, 70, 56,  1, 55, 56,  1, 62, 52,  1, 36, 52, 64, 54, 59,\n",
            "        52,  0, 28, 64,  1, 71, 64,  1, 62, 71, 58, 52, 68,  1, 55, 56,  1, 62,\n",
            "        52,  1, 36, 52, 64, 54, 59, 52,  7,  1, 55, 56,  1, 54, 71, 74, 65,  1,\n",
            "        64, 65, 63, 53, 68, 56,  1, 64, 65,  1, 67, 71, 60, 56, 68, 65,  1, 52,\n",
            "        54, 65, 68, 55, 52, 68, 63, 56,  7,  1, 64, 65,  1, 59, 52,  1, 63, 71,\n",
            "        54, 59, 65,  1, 70, 60, 56, 63, 66, 65,  1, 67, 71, 56,  1, 72, 60, 72,\n",
            "        60, 81, 52,  1, 71, 64,  1, 59, 60, 55, 52, 62, 58, 65,  1, 55, 56,  1,\n",
            "        62, 65, 69,  1, 55, 56,  1, 62, 52, 64, 75, 52,  1, 56, 64,  1, 52, 69,\n",
            "        70, 60, 62, 62, 56, 68, 65,  7,  1, 52, 55, 52, 68, 58, 52,  1, 52, 64,\n",
            "        70, 60, 58, 71, 52,  7,  1, 68, 65, 54, 60, 81, 64,  1, 57, 62, 52, 54,\n",
            "        65,  1, 74,  1, 58, 52, 62, 58, 65,  1, 54, 65, 68, 68, 56, 55, 65, 68,\n",
            "         9,  1, 44, 64, 52,  1, 65, 62, 62, 52,  1, 55, 56,  1, 52, 62, 58, 65,\n",
            "         1, 63, 52, 81, 69,  1, 72, 52, 54, 52,  1, 67, 71, 56,  1, 54, 52, 68,\n",
            "        64, 56, 68, 65,  7,  1, 69, 52, 62, 66, 60, 54, 65, 81, 64,  1, 62, 52,\n",
            "        69,  1, 63, 52, 81, 69,  1, 64, 65, 54, 59, 56, 69,  7,  1, 55, 71, 56,\n",
            "        62, 65, 69,  1, 74,  1, 67, 71, 56, 53, 68, 52, 64, 70, 65, 69,  1, 62,\n",
            "        65, 69,  1, 69, 52, 81, 53, 52, 55, 65, 69,  7,  1, 62, 56, 64, 70, 56,\n",
            "        61, 52, 69,  1, 62, 65, 69,  1, 72, 60, 56, 68, 64, 56, 69,  7,  1, 52,\n",
            "        62, 58, 71, 81, 64,  1, 66, 52, 62, 65, 63, 60, 64, 65,  1, 55, 56,  1,\n",
            "        52, 64, 82, 52, 55, 60, 55, 71, 68, 52,  1, 62, 65, 69,  1, 55, 65, 63,\n",
            "        60, 64, 58, 65, 69,  7,  1, 54, 65, 64, 69, 71, 63, 60, 81, 52, 64,  1,\n",
            "        62, 52, 69,  1, 70, 68, 56, 69,  1, 66, 52, 68, 70, 56, 69,  1, 55, 56,\n",
            "         1, 69, 71,  1, 59, 52, 54, 60, 56, 64, 55, 52,  9,  1, 28, 62,  1, 68,\n",
            "        56, 69, 70, 65,  1, 55, 56, 62, 62, 52,  1, 54, 65, 64, 54, 62, 71, 60,\n",
            "        81, 52, 64,  1, 69, 52, 74, 65,  1, 55, 56,  1, 72, 56, 62, 52, 68, 70,\n",
            "        56,  7,  1, 54, 52, 62, 75, 52, 69,  1, 55, 56,  1, 72, 56, 62, 62, 71,\n",
            "        55, 65,  1, 66, 52, 68, 52,  1, 62, 52, 69,  1, 57, 60, 56, 69, 70, 52,\n",
            "        69,  1, 54, 65, 64,  1, 69, 71, 69,  1, 66, 52, 64, 70, 71, 57, 62, 65,\n",
            "        69,  1, 55, 56,  1, 62, 65,  1, 63, 60, 69, 63, 65,  7,  1, 62, 65, 69,\n",
            "         1, 55, 60, 81, 52, 69,  1, 55, 56,  1, 56, 64, 70, 68, 56,  1, 69, 56,\n",
            "        63, 52, 64, 52,  1, 69, 56,  1, 59, 65, 64, 68, 52, 53, 52,  1, 54, 65,\n",
            "        64,  1, 69, 71,  1, 72, 56, 62, 62, 65, 68, 60,  1, 55, 56,  1, 62, 65,\n",
            "         1, 63, 52, 81, 69,  1, 57, 60, 64, 65,  9,  1, 43, 56, 64, 60, 81, 52,\n",
            "         1, 56, 64,  1, 69, 71,  1, 54, 52, 69, 52,  1, 71, 64, 52,  1, 52, 63,\n",
            "        52,  1, 67, 71, 56,  1, 66, 52, 69, 52, 53, 52,  1, 55, 56,  1, 62, 65,\n",
            "        69,  1, 54, 71, 52, 68, 56, 64, 70, 52,  7,  1, 74,  1, 71, 64, 52,  1,\n",
            "        69, 65, 53, 68, 60, 64, 52,  1, 67, 71, 56,  1, 64, 65,  1, 62, 62, 56,\n",
            "        58, 52, 53, 52,  1, 52,  1, 62, 65, 69,  1, 72, 56, 60, 64, 70, 56,  7,\n",
            "         1, 74,  1, 71, 64,  1, 63, 65, 75, 65,  1, 55, 56,  1, 54, 52, 63, 66,\n",
            "        65,  1, 74,  1, 66, 62, 52, 75, 52,  7,  1, 67, 71, 56,  1, 52, 69, 60,\n",
            "        81,  1, 56, 64, 69, 60, 62, 62, 52, 53, 52,  1, 56, 62,  1, 68, 65, 54,\n",
            "        60, 81, 64,  1, 54, 65, 63, 65,  1, 70, 65, 63, 52, 53, 52,  1, 62, 52,\n",
            "         1, 66, 65, 55, 52, 55, 56, 68, 52,  9,  1, 29, 68, 60, 69, 52, 53, 52,\n",
            "         1, 62, 52,  1, 56, 55, 52, 55,  1, 55, 56,  1, 64, 71, 56, 69, 70, 68,\n",
            "        65,  1, 59, 60, 55, 52, 62, 58, 65,  1, 54, 65, 64,  1, 62, 65, 69,  1,\n",
            "        54, 60, 64, 54, 71, 56, 64, 70, 52,  1, 52, 64, 82, 65, 69,  7,  1, 56,\n",
            "        68, 52,  1, 55, 56,  1, 54, 65, 63, 66, 62, 56, 73, 60, 65, 81, 64,  1,\n",
            "        68, 56, 54, 60, 52,  7,  1, 69, 56, 54])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separamos los datos en train y validation\n",
        "n = int(0.9*len(data)) # El primer 90% es train, el resto val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "7556266e-2f3a-4d05-ab94-5676f390294c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([27, 38, 37,  1, 40, 44, 32, 33, 38])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"Cuando entra {context} el target es: {decode([target.item()])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "7fc6efaf-0cdc-4b0e-fef7-8fbd9e49f75b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuando entra tensor([27]) el target es: O\n",
            "Cuando entra tensor([27, 38]) el target es: N\n",
            "Cuando entra tensor([27, 38, 37]) el target es:  \n",
            "Cuando entra tensor([27, 38, 37,  1]) el target es: Q\n",
            "Cuando entra tensor([27, 38, 37,  1, 40]) el target es: U\n",
            "Cuando entra tensor([27, 38, 37,  1, 40, 44]) el target es: I\n",
            "Cuando entra tensor([27, 38, 37,  1, 40, 44, 32]) el target es: J\n",
            "Cuando entra tensor([27, 38, 37,  1, 40, 44, 32, 33]) el target es: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # Cuántas secuencias en paralelo?\n",
        "block_size = 8 # Cuál es el largo máximo para predecir?\n",
        "\n",
        "def get_batch(split):\n",
        "    # Preparamos un batch pequeño de datos: inputs x --- targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"Cuando entra {context.tolist()} sale: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4d8625fc-7c5a-4f79-ff13-15d7c91fc136"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[70, 52, 55,  1, 55, 56,  1, 66],\n",
            "        [ 7,  1, 54, 65, 64,  1, 56, 62],\n",
            "        [60,  1, 52,  1, 63, 60, 81,  1],\n",
            "        [54, 52, 68, 52,  7,  1, 64, 60]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 55,  1, 55, 56,  1, 66, 52],\n",
            "        [ 1, 54, 65, 64,  1, 56, 62,  1],\n",
            "        [ 1, 52,  1, 63, 60, 81,  1, 63],\n",
            "        [52, 68, 52,  7,  1, 64, 60,  1]])\n",
            "----\n",
            "Cuando entra [70] sale: 52\n",
            "Cuando entra [70, 52] sale: 55\n",
            "Cuando entra [70, 52, 55] sale: 1\n",
            "Cuando entra [70, 52, 55, 1] sale: 55\n",
            "Cuando entra [70, 52, 55, 1, 55] sale: 56\n",
            "Cuando entra [70, 52, 55, 1, 55, 56] sale: 1\n",
            "Cuando entra [70, 52, 55, 1, 55, 56, 1] sale: 66\n",
            "Cuando entra [70, 52, 55, 1, 55, 56, 1, 66] sale: 52\n",
            "Cuando entra [7] sale: 1\n",
            "Cuando entra [7, 1] sale: 54\n",
            "Cuando entra [7, 1, 54] sale: 65\n",
            "Cuando entra [7, 1, 54, 65] sale: 64\n",
            "Cuando entra [7, 1, 54, 65, 64] sale: 1\n",
            "Cuando entra [7, 1, 54, 65, 64, 1] sale: 56\n",
            "Cuando entra [7, 1, 54, 65, 64, 1, 56] sale: 62\n",
            "Cuando entra [7, 1, 54, 65, 64, 1, 56, 62] sale: 1\n",
            "Cuando entra [60] sale: 1\n",
            "Cuando entra [60, 1] sale: 52\n",
            "Cuando entra [60, 1, 52] sale: 1\n",
            "Cuando entra [60, 1, 52, 1] sale: 63\n",
            "Cuando entra [60, 1, 52, 1, 63] sale: 60\n",
            "Cuando entra [60, 1, 52, 1, 63, 60] sale: 81\n",
            "Cuando entra [60, 1, 52, 1, 63, 60, 81] sale: 1\n",
            "Cuando entra [60, 1, 52, 1, 63, 60, 81, 1] sale: 63\n",
            "Cuando entra [54] sale: 52\n",
            "Cuando entra [54, 52] sale: 68\n",
            "Cuando entra [54, 52, 68] sale: 52\n",
            "Cuando entra [54, 52, 68, 52] sale: 7\n",
            "Cuando entra [54, 52, 68, 52, 7] sale: 1\n",
            "Cuando entra [54, 52, 68, 52, 7, 1] sale: 64\n",
            "Cuando entra [54, 52, 68, 52, 7, 1, 64] sale: 60\n",
            "Cuando entra [54, 52, 68, 52, 7, 1, 64, 60] sale: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # La entrada de nuestro transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "fe1500d0-1a5a-4244-d841-c5dc5b0c7113"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[70, 52, 55,  1, 55, 56,  1, 66],\n",
            "        [ 7,  1, 54, 65, 64,  1, 56, 62],\n",
            "        [60,  1, 52,  1, 63, 60, 81,  1],\n",
            "        [54, 52, 68, 52,  7,  1, 64, 60]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Cada token lee directamente los logits del siguiente desde esta tabla\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx y targets son ambos (B,T) tensores de enteros\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx es (B, T) array de indices en el contexto actual\n",
        "        for _ in range(max_new_tokens):\n",
        "            # obtener las predictions\n",
        "            logits, loss = self(idx)\n",
        "            # foco en el último paso\n",
        "            logits = logits[:, -1, :] # se vuelve (B, C)\n",
        "            # applicar softmax\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample desde la distribucion\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append index sampleado a la secuencia en ejecución\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "09c72254-a76e-4f3e-e0f3-0512f9ee2cd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 89])\n",
            "tensor(5.2697, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "vWUAF]”0lUz;eW0[xo'1IXOV(iO7LyE.rstT(d“l;8Xa3b4H7\"d̈:,dulhXS6.M2(rz¡̈¡9GfN!(Kf5¿Q6BRBdOqg3,AO]D\n",
            "N’L–\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos un optimizador\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # más pasos, mejores resultados\n",
        "\n",
        "    # sampleamos un batch\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluamos el error\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "ead0b78a-b349-44bf-8dd4-8fdcd35cc988"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.858426094055176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "744c1059-0680-4c40-911e-a7199015dc67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8<dS??<o9i‘8v-iiNip–itQ;4.Q'́\"e!X2Ds5YXHY5q̀5ed̈dXHYERgub¿G[NJP‘yM9¿31–“.]a“¡«7!yy[̈\n",
            "yGK!–”!K–\"T‘?['–u)KOs(Lh»aHy RgCG6iZD;:BXTre6iKzJ–3mOKj0],4Nz\n",
            "caH]41tVE.Jp̈:('̈b¿6 Bn:Lt-“¡̃'–jy¡!1Ru\n",
            "6tW.6«l'oN:7FOEV́vf6<a¿»8«41g,Sl1\n",
            "[a”̈:l4PG9o?non<SgLq<XLvLHYzU–“:3ORCst-nfg<́”)Id<́uiI,¡ziLn Be¡MV'GPvWTM4H̃DM’Aufg8\"bn8\"d1‘YaoOr0a:RMVMQ8eW!B̃7źOD¿B‘̈¡I3(¡h\n",
            "̀HmWiMpLed̈7¿;\n",
            "dVV'’m\n",
            "Eb]¿QLEV Z'41́,S?“‘(P‘fa“Y5u«x“Fc1U)Vyl0s(8vpBũg'7J!lfD?C<EM9yMpP0J(zGnd’́M h“TDn'-DNe)Bxb‘O.0L?x?PVT(rE4N:RM;pzKRb¿q3mno3h8MVrOvu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=600)[0].tolist()))\n",
        "\n",
        "# Sí, yo creo que anda joya... pero no lo entrenamos, no?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGrHP-ihJdYb",
        "outputId": "453fd85a-3a17-4aeb-b6e5-005a6ed00aa3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "H 3O.H5‘PUW6H́l4:F?”zSfrD́la“¡.)uM1JTCS?x2’7EćRd’́R5zyv,SYsKdr:p¿¡<RRhc<oN‘Cd5-\n",
            "; MP;QL?,̀WU8CF<bs(r.u:.Q<.‘“3PĨP;CEV?6'\"9–1GL'h7f0–ZpNJ2qd4uBRmQ̈W4–]a“e(TJ\"ë¿:ODB.‘l3”2‘;[-¡XCHc‘5–EcuM'́ECG;emOUzaIYld6́6́‘qGWlFgX:zXTd“uQ l',Sinzt]K“c\n",
            "rSgz0m̃A»B-6aGpl37RCpe m»3(yE”[̈7LLd»jBJ3bO?–N:–:‘ai‘GLv6–Ejfm :“'–? q.l'́N-67u<!¡“”c1̀Kd.mRlz-5IOK–JT’́ljH’8r[ć;]<̀PXT<D¿5btzKco'Ax–3qcaW̃(àq‘?Tf(uxZ¿Q2S»\"QYa!692,SluaI<(H.6‘\"<?[f<dv)s6C9,)p¿Y«Gj?-:]dOzVsO\n",
            "ibYzbp:5DH]q‘“:eyEV“Y’VI)y!,̈-<TCMCj!nF<‘¡SD5̈h0<RB9Jp7<bv́<0qLcaNlx2̈f]caT?xb3cY;:5̈8ClD»–Vm3̈?[<j!??lz‘P;Oz)A;O\n",
            "9WiMpo?¡̃;fW2p–LO gYWmEu’(̃ J36“eBBZ\"Ta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- La atención es un mecanismo de comunicación. Puede entenderse como nodos en un grafo dirigido, que se apuntan mutuamente y van agregando informacio´n ponderada con una suma pesada de todos los nodos que lo apuntan.\n",
        "- El modelo no tiene noción de espacio. Hay atención sobre un conjunto de vectroes. Necesitamos codificar los tokens posicionalmente.\n",
        "- Cada dimensión se procesa independientemente.\n",
        "- Si es un bloque encoder, se saca el enmascaramiento `tril`, así todos los tokens se comunican. Un decoder tiene enmascaramiento triangular, y ese usa en contextos autorregresivos (modelado de lenguaje).\n",
        "- \"self-attention\" significa que las keys y values se producen igual que las queries. \"cross-attention\": las queries se producen desde x, pero las keys y values vienen de otra fuente (módulo encoder).\n",
        "- Si es \"Scaled\" attention, se divide `wei` por 1/sqrt(head_size). Esto hace que cuando la entrada Q,K respeta unit variance, wei también la respetará, y la softmax seguirá \"difusa\" y no saturará (mucho)."
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MicroGPT:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 9000\n",
        "#max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt\n",
        "\n",
        "with open('el_quijote.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "27c33cc7-f564-4ec0-a2b7-036d5718cc4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-20 01:01:43--  https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1060259 (1.0M) [text/plain]\n",
            "Saving to: ‘el_quijote.txt.3’\n",
            "\n",
            "\rel_quijote.txt.3      0%[                    ]       0  --.-KB/s               \rel_quijote.txt.3    100%[===================>]   1.01M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-20 01:01:43 (31.5 MB/s) - ‘el_quijote.txt.3’ saved [1060259/1060259]\n",
            "\n",
            "0.212825 M parameters\n",
            "step 0: train loss 4.6837, val loss 4.6888\n",
            "step 100: train loss 2.3703, val loss 2.4025\n",
            "step 200: train loss 2.2507, val loss 2.2982\n",
            "step 300: train loss 2.1992, val loss 2.2474\n",
            "step 400: train loss 2.1578, val loss 2.1908\n",
            "step 500: train loss 2.1007, val loss 2.1226\n",
            "step 600: train loss 2.0657, val loss 2.1029\n",
            "step 700: train loss 2.0074, val loss 2.0513\n",
            "step 800: train loss 1.9784, val loss 2.0262\n",
            "step 900: train loss 1.9330, val loss 1.9832\n",
            "step 1000: train loss 1.9032, val loss 1.9606\n",
            "step 1100: train loss 1.8820, val loss 1.9359\n",
            "step 1200: train loss 1.8496, val loss 1.9027\n",
            "step 1300: train loss 1.8272, val loss 1.8818\n",
            "step 1400: train loss 1.8081, val loss 1.8795\n",
            "step 1500: train loss 1.7837, val loss 1.8398\n",
            "step 1600: train loss 1.7727, val loss 1.8355\n",
            "step 1700: train loss 1.7531, val loss 1.8435\n",
            "step 1800: train loss 1.7449, val loss 1.8100\n",
            "step 1900: train loss 1.7196, val loss 1.7722\n",
            "step 2000: train loss 1.7091, val loss 1.7809\n",
            "step 2100: train loss 1.6935, val loss 1.7736\n",
            "step 2200: train loss 1.6932, val loss 1.7449\n",
            "step 2300: train loss 1.6739, val loss 1.7522\n",
            "step 2400: train loss 1.6661, val loss 1.7319\n",
            "step 2500: train loss 1.6619, val loss 1.7264\n",
            "step 2600: train loss 1.6324, val loss 1.7340\n",
            "step 2700: train loss 1.6327, val loss 1.7075\n",
            "step 2800: train loss 1.6254, val loss 1.7054\n",
            "step 2900: train loss 1.6283, val loss 1.7031\n",
            "step 3000: train loss 1.6077, val loss 1.6974\n",
            "step 3100: train loss 1.5978, val loss 1.6792\n",
            "step 3200: train loss 1.5857, val loss 1.6767\n",
            "step 3300: train loss 1.5839, val loss 1.6726\n",
            "step 3400: train loss 1.5879, val loss 1.6580\n",
            "step 3500: train loss 1.5893, val loss 1.6543\n",
            "step 3600: train loss 1.5764, val loss 1.6729\n",
            "step 3700: train loss 1.5733, val loss 1.6618\n",
            "step 3800: train loss 1.5447, val loss 1.6493\n",
            "step 3900: train loss 1.5652, val loss 1.6530\n",
            "step 4000: train loss 1.5465, val loss 1.6452\n",
            "step 4100: train loss 1.5301, val loss 1.6189\n",
            "step 4200: train loss 1.5466, val loss 1.6281\n",
            "step 4300: train loss 1.5458, val loss 1.6304\n",
            "step 4400: train loss 1.5320, val loss 1.6337\n",
            "step 4500: train loss 1.5243, val loss 1.6209\n",
            "step 4600: train loss 1.5202, val loss 1.6070\n",
            "step 4700: train loss 1.5213, val loss 1.6140\n",
            "step 4800: train loss 1.5130, val loss 1.6164\n",
            "step 4900: train loss 1.5144, val loss 1.5934\n",
            "step 5000: train loss 1.5042, val loss 1.6017\n",
            "step 5100: train loss 1.4973, val loss 1.5913\n",
            "step 5200: train loss 1.5059, val loss 1.5921\n",
            "step 5300: train loss 1.4985, val loss 1.5849\n",
            "step 5400: train loss 1.4951, val loss 1.5945\n",
            "step 5500: train loss 1.4817, val loss 1.5945\n",
            "step 5600: train loss 1.4939, val loss 1.5861\n",
            "step 5700: train loss 1.4835, val loss 1.5774\n",
            "step 5800: train loss 1.4876, val loss 1.5811\n",
            "step 5900: train loss 1.4790, val loss 1.5586\n",
            "step 6000: train loss 1.4962, val loss 1.5797\n",
            "step 6100: train loss 1.4710, val loss 1.5843\n",
            "step 6200: train loss 1.4764, val loss 1.5911\n",
            "step 6300: train loss 1.4727, val loss 1.5758\n",
            "step 6400: train loss 1.4644, val loss 1.5639\n",
            "step 6500: train loss 1.4638, val loss 1.5796\n",
            "step 6600: train loss 1.4787, val loss 1.5730\n",
            "step 6700: train loss 1.4792, val loss 1.5774\n",
            "step 6800: train loss 1.4582, val loss 1.5523\n",
            "step 6900: train loss 1.4615, val loss 1.5640\n",
            "step 7000: train loss 1.4510, val loss 1.5611\n",
            "step 7100: train loss 1.4490, val loss 1.5549\n",
            "step 7200: train loss 1.4610, val loss 1.5661\n",
            "step 7300: train loss 1.4592, val loss 1.5590\n",
            "step 7400: train loss 1.4497, val loss 1.5582\n",
            "step 7500: train loss 1.4473, val loss 1.5514\n",
            "step 7600: train loss 1.4436, val loss 1.5470\n",
            "step 7700: train loss 1.4424, val loss 1.5457\n",
            "step 7800: train loss 1.4512, val loss 1.5351\n",
            "step 7900: train loss 1.4405, val loss 1.5404\n",
            "step 8000: train loss 1.4443, val loss 1.5550\n",
            "step 8100: train loss 1.4325, val loss 1.5586\n",
            "step 8200: train loss 1.4402, val loss 1.5561\n",
            "step 8300: train loss 1.4387, val loss 1.5542\n",
            "step 8400: train loss 1.4293, val loss 1.5488\n",
            "step 8500: train loss 1.4278, val loss 1.5377\n",
            "step 8600: train loss 1.4218, val loss 1.5123\n",
            "step 8700: train loss 1.4313, val loss 1.5389\n",
            "step 8800: train loss 1.4291, val loss 1.5556\n",
            "step 8900: train loss 1.4182, val loss 1.5361\n",
            "step 8999: train loss 1.4238, val loss 1.5366\n",
            "\n",
            "que la mara a persona que, él\n",
            "todos los de razónde, vino por tal tienego en mi él, guardado, como te descubrirado comndaba y bien, con que desdichadados, porque los damas? ¿Podrar detingenciados a allí, petar contena que habírse tantos porque algúnso me sitiase a estoy la infada de don Quijote; bien fin he dicho. ¿Todo, que ni se hable de caballo, lo que le parece como pregunta saben de mi procura, que se es roscorien?, le veces de su bebico de decir a la otra cosa, así el vuelto, que el saltaño y el que pasto aquellos día yo de la razón de la soluna, y cantido el saldrio, que tomaras merced, porque sino delante sus casoritas, ni vista y bien asiguado.\n",
            "-Micómeta, por la locura, que en tan guarrme que cotins cabreros, que de atoncles por aquellos, no hiciera de más se impio que tar el Caballo; y escogendor su hermoso los dudozías y añado, me cuanto me la abazaba de allí, de la ventable del Toboso, y no fue de lo que con en Don Quijote. jenanse imbulparas a su desgrado todo estaba de nuestros, y profanteces y Bardela, no restante Panza al dondes le reindo aquísito que se ponsaba, apreso los implicos el que atonces a esto, y talta a; y le rogólto de los imaginarías, será que un gente corle, la niaciósicer, en allí argua Migún prepio, por aconterle el qué entender las nuevas que el dió. Díjole, ¿de aquello que los cadas, señoras que impuñas de apidamo padrante a del rato allí a la atídilura de ahora quitarte, amarar en su birfe, y porque en veré por acomodo su sones. Porno, pensó visto, y otros oírse aquí no me escuverse la mantera, de excuder molo ocindo a los Asembas en y camantos! Lotan, a ejecu- yo cosarion; y puede de los crituros bien, ninguna gran; no sobre entoca, alguna puensante de la rocedeza con la nocha y estaba tesorme de ser razobres; y por uno nímo es verdad, e brazo dencar en camino nos sé lenguada de achernos me don Quijote; el cual admirezar a la sobrado que como mie hado así a ¿extimagos, al corrazón un laño\n",
            "Port\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0275b6fc-6f4e-45bf-b95f-080de46c9d7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-1quien que él diced que su ni respondía Dorotea. ¡Aho? Hecha podía, y por agusaré, cercala en la provechada en grado MachaOY cómo era de sacas, aunque yo soy bajados de que acaban, y de me buenos demagase echaramentes reduplia. Yo esto, le hace aragoltados que acertosa haciendos perosable salio, que no tomar en el cuestrada, a desdad quiso de tiern razo se puedez que tomarrados colices de Amohecharse hallaba vio presisa. Porque ibajababa desta pala.\n",
            "Y coyo? sosobre de caldenga, huéndad de más desamos -oso quitar muchos entrovechos su exto, el y ni haclaría en limpo) a saber. Todóse lo que declario de la bocadecnta y este en el otro, puesto me sobre haceder conga, lo que otra avio, resió hubierse le vuestrido si no fles, entenderó y él dado entrecho y porque no las más acobre con gran los la ventideros de la vecebida, crestamente los de Triste con él, -respresto que su señores y del pocor de margos han de habían poco podrán, yo te quiego, los dos yelmas el verdade ducinezs, que deshace a faltarle con defenderse el que ofrezal, pramienza y del rodo los soltirís, apadmero toca y comien. Cualarse, y aunque me veces sucedecillado.\n",
            "Esto esto discretos es Lustara, y aflirso en los señoras de jamás a mis de brazo y los pagañan por escuchidas seframentes de la vocans amoras fle, esposaderos.\n",
            "-Conjeron Panza mis qué loco; si llana que la solguno necesidad y perganzas que así la cida, que yo vallás acabada, como cómo en lo delcían que otras porque sabia y no hay añatica, ni a dándose le ciminaron de mi cautillo; dilso, nuevos dicellos fortana con sus manos, vienen contornos digo de Macientes caballero que allí a Zoraida Sancho Paraido; pero, impuesto que memor de enver para que con esto acomo de verdader de sus nombre de la razón el porligrosa a encaste pagdenan, pudió que hace!\n",
            "-¿Quién haya fuega, no más la risparse de las calzas añas, comenzado, por medio será tarla de que allí dilas que se había el paciedo por la disparate que no sobre \n"
          ]
        }
      ]
    }
  ]
}